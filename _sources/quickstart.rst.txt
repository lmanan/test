=============
Quick Start
=============


.. automodule:: cellulus
  :noindex:

:jupyter-download-notebook:`Download this page as a Jupyter notebook<quickstart>`

In this tutorial, we will use Cellulus to segment gray-scale images of **Fluo-N2DL-HeLa** available on the `Cell Tracking Challenge <http://celltrackingchallenge.net/2d-datasets/>`_ webpage.

Here below you see an example raw image on the left and the desired segmentation on the right.

|pic1| |pic2|

.. |pic1| image:: ./_static/quickstart/t045.png
   :width: 45%

.. |pic2| image:: ./_static/quickstart/man_seg045.png
   :width: 45%

Firstly, a subset of the original *tif* raw images are downloaded.

.. jupyter-execute::
 
  name = "Fluo-N2DL-HeLa" 
  data_dir = "./data"
  
  import cellulus
  from cellulus.utils.misc import extract_data
  extract_data(
    zip_url="https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/Fluo-N2DL-HeLa.zip",
    data_dir=data_dir,
    project_name=name,
  )


Next, these raw images are intensity-normalized and saved as a *zarr* file.

.. jupyter-execute::

  import numpy as np, zarr, tifffile, os
  from tqdm import tqdm
  from glob import glob
  from csbdeep.utils import normalize

  container_path = zarr.open(name+".zarr")
  dataset_name = 'train/raw'
  image_filenames = sorted(glob(os.path.join(data_dir, name, "images/*.tif")))[:10]
  print(f"Number of raw images is {len(image_filenames)}")
  image_list = []

  for i in tqdm(range(len(image_filenames))):
    im = normalize(tifffile.imread(image_filenames[i]).astype(np.float32), pmin =1, pmax= 99.8, axis = (0,1))
    image_list.append(im[np.newaxis, ...])

  image_list = np.asarray(image_list) # adds an extra 'batch ' dim
  container_path[dataset_name] = image_list
  container_path[dataset_name].attrs["resolution"] = (1, 1)
  container_path[dataset_name].attrs["axis_names"] = ("s", "c", "y", "x")


Next, let us specify configuration parameters. 

Firstly, let us specify the `train_data_config` which identifies where the data lives.

.. jupyter-execute::

  train_data_config = {'container_path': name+'.zarr', 
                       'dataset_name': dataset_name
                      }


Next, let us specify the `model_config` which identifies the structure of the network we will train. 
Here, we specified the number of feature maps `num_fmaps` equal to `24`. Feel free to set this to a higher number for a larger network.

.. jupyter-execute::

  num_fmaps = 24
  fmap_inc_factor = 3

  model_config= {'num_fmaps': num_fmaps,
                 'fmap_inc_factor': fmap_inc_factor,
                 }

Then, we specify the `train_config` which identifies some parameters of how the training process would go, for example, which device to run the training on, number of iterations to train for etc.
The device could be set equal to 'cuda:n' (where n is the index of the GPU, for e.g. 'cuda:0'), 'cpu' or 'mps'. 

.. jupyter-execute::

  train_config = {'train_data_config': train_data_config,
                  'max_iterations': 5_000,
                  'save_model_every': 100,
                  'device': 'mps'
                 }

Lastly, we specify `experiment_config` which contains our `model_config` and `train_config` (initialized above).

.. jupyter-execute::
  
  from cellulus.configs.experiment_config import ExperimentConfig
  experiment_config = ExperimentConfig(model_config = model_config, 
                                       train_config = train_config)
 
Now we can begin the training. 

.. jupyter-execute::
  :hide-output:
   
  from cellulus.train import train
  train(experiment_config)

Now that the training is over, let us apply the trained model weights to infer embeddings on this data.
To do so, we again specify the `dataset_config` which identifies the data which needs to be segmented.

.. jupyter-execute::

  dataset_config = {'container_path': name +'.zarr', 
                    'dataset_name': dataset_name
                   }

Next, we specify the `prediction_dataset_config` which contains the embeddings and the uncertainty predicted per pixel.

.. jupyter-execute::
  
  prediction_dataset_config = {'container_path': name + '.zarr',
                               'dataset_name': 'embedding'
                              }


Then, we specify the `segmentation_dataset_config` which contains the instance segmentations generated by clustering the predicted embeddings.

.. jupyter-execute::

  segmentation_dataset_config = {'container_path': name + '.zarr',
                                 'dataset_name': 'segmentation' 
                                }

Then we specify the `postprocessed_dataset_config`, which contains the results of some post-processing of the instance segmentations, for e.g. removing any objects which are smaller than a certain size threhold.

.. jupyter-execute::

  post_processed_dataset_config = {'container_path': name + '.zarr',
                                   'dataset_name': 'post_processed_segmentation' 
                                  }


Next, we specify `inference_config` which contains our `prediction_dataset_config`, `segmentation_dataset_config` and `post_processed_dataset_config` (initialized above).


.. jupyter-execute::

   inference_config = {'dataset_config': dataset_config,
                       'prediction_dataset_config': prediction_dataset_config,
                       'segmentation_dataset_config': segmentation_dataset_config,
                       'post_processed_dataset_config': post_processed_dataset_config,
                       'device': 'mps',
                      }

Also, we must specify the `model_config` and direct the checkpoint to be the *best* weights (*best* in terms of the lowest loss obtained). 

.. jupyter-execute::

  model_config= {'num_fmaps': num_fmaps,
                 'fmap_inc_factor': fmap_inc_factor,
                 'checkpoint': 'models/best_loss.pth'
                }

  
Lastly, we specify `experiment_config` which contains our `model_config` and `train_config` (initialized above).

.. jupyter-execute::
  
  from cellulus.configs.experiment_config import ExperimentConfig
  experiment_config = ExperimentConfig(model_config = model_config, 
                                      inference_config = inference_config)



Now we are ready to start the inference:

.. jupyter-execute::
   
  from cellulus.infer import infer
  infer(experiment_config)

Let's look at some of the predicted embeddings. Change the `index` below to look at the raw image (left), x-offset (bottom-left), y-offset (bottom-right) and standard deviation of the embedding (top-right). 

.. jupyter-execute::

  from cellulus.utils.misc import visualize_2d

  index = 0

  f = zarr.open(name+ '.zarr')
  ds = f['train/raw']
  ds2 = f['embedding']
  
  image = ds[index, 0]
  embedding = ds2[index]

  visualize_2d(image, top_right = embedding[-1], bottom_left=embedding[0], bottom_right=embedding[1],
              top_right_label = "STD_DEV", bottom_left_label="OFFSET_X", bottom_right_label="OFFSET_Y")

As you can see the standard deviation of the magnitude of the embedding is low for most of the foreground cells. This enables extraction of the foreground, which is clustered into individual instances.


.. jupyter-execute::

  from cellulus.utils.misc import visualize_2d
  import skimage

  index = 0

  f = zarr.open(name+ '.zarr')
  ds = f['train/raw']
  ds2 = f['segmentation']
  ds3 = f['post_processed_segmentation']

  visualize_2d(image, top_right = embedding[-1] < skimage.filters.threshold_otsu(embedding[-1]), 
              bottom_left = ds2[0, 0], bottom_right = ds3[0, 0],
              top_right_label = "THRESHOLDED F.G.", bottom_left_label="SEGMENTATION", 
              bottom_right_label="POSTPROCESSED")
  
